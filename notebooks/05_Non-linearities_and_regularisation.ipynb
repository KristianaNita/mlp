{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "\n",
    "In this lab we will explore different methods for regularising networks to reduce overfitting and improve generalisation. This uses the material covered in the [fourth lecture slides](http://www.inf.ed.ac.uk/teaching/courses/mlp/2018-19/mlp04-reg.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: L1 and L2 penalties\n",
    "\n",
    "In the previous lab notebook we explored the issue of overfitting. There we saw that this arises when the model is 'too complex' ($\\sim$ has too many degrees of freedom / parameters) for the amount of data we have available.\n",
    "\n",
    "One method for trying to reduce overfitting is therefore to try to decrease the flexibility of the model. We can do this by simply reducing the number of free parameters in the model (e.g. by using a shallower model with fewer layers or layers with smaller dimensionality). More generally however we might want some way of more continuously varying the effective flexibility of a model with a fixed architecture.\n",
    "\n",
    "A common method for doing this is to add an additional term to the objective function being minimised during training which penalises some measure of the complexity of a model as a function of the model parameters. The aim of training is then to minimise with respect to the model parameters the sum $E^\\star$ of the data-driven error function term $\\bar{E}$ and a model complexity term $C$.\n",
    "\n",
    "\\begin{equation}\n",
    "   E^\\star =\n",
    "   \\underbrace{\\bar{E}}_{\\textrm{data term}} + \\underbrace{C}_{\\textrm{complexity term}}\n",
    "\\end{equation}\n",
    "\n",
    "We need the complexity term $C$ to be easy to compute and differentiable with respect to the model parameters. A common choice is to use terms involving the *norms* ($\\sim$ a measure of size) of the parameters. This penalises models with large parameter values. Two commonly used norms are the L1 and L2 norms. \n",
    "\n",
    "For a $D$ dimensional vector $\\mathbf{v}$ the L1 norm is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\| \\boldsymbol{v} \\|_1 = \\sum_{d=1}^D \\left| v_d \\right|,\n",
    "\\end{equation}\n",
    "\n",
    "and the L2 norm is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\| \\boldsymbol{v} \\|_2 = \\left[ \\sum_{d=1}^D \\left( v_d^2 \\right) \\right]^{\\frac{1}{2}}.\n",
    "\\end{equation}\n",
    "\n",
    "For a $K \\times D$ matrix $\\mathbf{M}$, we will define norms by collapsing the matrix to a vector $\\boldsymbol{m} = \\mathrm{vec}\\left[\\mathbf{M}\\right] = \\left[ M_{1,1} \\dots M_{1,D} ~ M_{2,1} \\dots M_{K,D} \\right]^{\\rm T}$ and then taking the norm as defined above of this resulting vector (practically this just results in summing over two sets of indices rather than one).\n",
    "\n",
    "The overall complexity penalty term $C$ is defined as a sum over individual complexity terms for each of the $P$ parameters of the model \n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\sum_{i=1}^P \\left[ C^{(i)} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Some of these per-parameter penalty terms $C^{(i)}$ may be set to zero if we do not wish to penalise the size of the corresponding parameter.\n",
    "\n",
    "To enable us to tradeoff between the model complexity and data error terms, it is typical to introduce positive scalar coefficients $\\beta_i$ to scale the penalty term on the $i$th parameter. A *L1 penalty* on the $i$th vector parameter $\\boldsymbol{p}^{(i)}$ (or matrix parameter collapsed to a vector) is then commonly defined as\n",
    "\n",
    "\\begin{equation}\n",
    "  C^{(i)}_{\\textrm{L1}} = \n",
    "  \\beta_i \\left\\| \\boldsymbol{p}^{(i)} \\right\\|_1 = \n",
    "  \\beta_i  \\sum_{d=1}^D \\left| p^{(i)}_d \\right|.\n",
    "\\end{equation}\n",
    "\n",
    "This has a gradient with respect to the parameter vector\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{(i)}_{\\textrm{L1}}}{\\partial p^{(i)}_d} = \\beta_i \\, \\textrm{sgn}\\left( p^{(i)}_d \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\textrm{sgn}(u) = +1$ if $u > 0$, $\\textrm{sgn}(u) = -1$ if $u < 0$ (and is not well defined for $u=0$ though a common convention is to have $\\textrm{sgn}(0) = 0$).\n",
    "\n",
    "Similarly a *L2 penalty* on the $i$th vector parameter $\\boldsymbol{p}^{(i)}$ (or matrix parameter collapsed to a vector) is commonly defined as\n",
    "\n",
    "\\begin{equation}\n",
    "  C^{(i)}_{\\textrm{L2}} = \n",
    "  \\frac{1}{2} \\beta_i \\left\\| \\boldsymbol{p}^{(i)} \\right\\|_2^2 =\n",
    "  \\frac{1}{2} \\beta_i \\sum_{d=1}^D \\left[ \\left( p^{(i)}_d \\right)^2 \\right].\n",
    "\\end{equation}\n",
    "\n",
    "Somewhat confusingly this is proportional to the square of the L2 norm rather than the L2 norm itself, however it is an almost universal convention to call this an L2 penalty so we will stick with this nomenclature here. The $\\frac{1}{2}$ term is less universal and is sometimes not included; we include it here for consistency with how we defined the sum of squared errors cost. Similarly to that case, the $\\frac{1}{2}$ cancels when calculating the gradient with respect to the parameter\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{(i)}_{\\textrm{L2}}}{\\partial p^{(i)}_d} = \\beta_i p^{(i)}_d\n",
    "\\end{equation}\n",
    "\n",
    "Use the above definitions for the L1 and L2 penalties for a parameter and corresponding gradients to implement the `__call__` and `grad` methods respectively for the skeleton `L1Penalty` and `L2Penalty` class definitions below. The `coefficient` property of these classes should be used as the $\\beta_i$ value in the equations above. The parameter the penalty term (or gradient) is being evaluated for will be either a one or two-dimensional NumPy array (corresponding to a vector or matrix parameter respectively) and your implementations should be able to cope with both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 22102017 \n",
    "rng = np.random.RandomState(seed)\n",
    "class L1Penalty(object):\n",
    "    \"\"\"L1 parameter penalty.\n",
    "    \n",
    "    Term to add to the objective function penalising parameters\n",
    "    based on their L1 norm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, coefficient):\n",
    "        \"\"\"Create a new L1 penalty object.\n",
    "        \n",
    "        Args:\n",
    "            coefficient: Positive constant to scale penalty term by.\n",
    "        \"\"\"\n",
    "        assert coefficient > 0., 'Penalty coefficient must be positive.'\n",
    "        self.coefficient = coefficient\n",
    "        \n",
    "    def __call__(self, parameter):\n",
    "        \"\"\"Calculate L1 penalty value for a parameter.\n",
    "        \n",
    "        Args:\n",
    "            parameter: Array corresponding to a model parameter.\n",
    "            \n",
    "        Returns:\n",
    "            Value of penalty term.\n",
    "        \"\"\"\n",
    "        param_vector = parameter.reshape(-1, 1)\n",
    "        l1_penalty = self.coefficient * np.sum(np.absolute(param_vector))\n",
    "        return l1_penalty\n",
    "        \n",
    "        \n",
    "    def grad(self, parameter):\n",
    "        \"\"\"Calculate the penalty gradient with respect to the parameter.\n",
    "        \n",
    "        Args:\n",
    "            parameter: Array corresponding to a model parameter.\n",
    "            \n",
    "        Returns:\n",
    "            Value of penalty gradient with respect to parameter. This\n",
    "            should be an array of the same shape as the parameter.\n",
    "        \"\"\"\n",
    "        return self.coefficient * np.sign(parameter)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'L1Penalty({0})'.format(self.coefficient)\n",
    "        \n",
    "\n",
    "class L2Penalty(object):\n",
    "    \"\"\"L1 parameter penalty.\n",
    "    \n",
    "    Term to add to the objective function penalising parameters\n",
    "    based on their L2 norm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, coefficient):\n",
    "        \"\"\"Create a new L2 penalty object.\n",
    "        \n",
    "        Args:\n",
    "            coefficient: Positive constant to scale penalty term by.\n",
    "        \"\"\"\n",
    "        assert coefficient > 0., 'Penalty coefficient must be positive.'\n",
    "        self.coefficient = coefficient\n",
    "        \n",
    "    def __call__(self, parameter):\n",
    "        \"\"\"Calculate L2 penalty value for a parameter.\n",
    "        \n",
    "        Args:\n",
    "            parameter: Array corresponding to a model parameter.\n",
    "            \n",
    "        Returns:\n",
    "            Value of penalty term.\n",
    "        \"\"\"\n",
    "        param_vector = parameter.reshape(-1, 1)\n",
    "        l2_penalty = self.coefficient/2 * np.sum(param_vector**2)\n",
    "        return l2_penalty\n",
    "        \n",
    "        \n",
    "    def grad(self, parameter):\n",
    "        \"\"\"Calculate the penalty gradient with respect to the parameter.\n",
    "        \n",
    "        Args:\n",
    "            parameter: Array corresponding to a model parameter.\n",
    "            \n",
    "        Returns:\n",
    "            Value of penalty gradient with respect to parameter. This\n",
    "            should be an array of the same shape as the parameter.\n",
    "        \"\"\"\n",
    "        return self.coefficient * parameter\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'L2Penalty({0})'.format(self.coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementations by running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test values calculated correctly for L1Penalty\n"
     ]
    }
   ],
   "source": [
    "test_params_1 = np.array([[0.5, 0.3, -1.2, 5.8], [0.2, -3.1, 4.9, -5.0]])\n",
    "test_params_2 = np.array([0.8, -0.6, -0.3, 1.5, 2.8])\n",
    "true_l1_cost_1 = 10.5\n",
    "true_l1_grad_1 = np.array([[0.5, 0.5, -0.5, 0.5], [0.5, -0.5, 0.5, -0.5]])\n",
    "true_l1_cost_2 = 3.\n",
    "true_l1_grad_2 = np.array([0.5, -0.5, -0.5, 0.5, 0.5])\n",
    "l1 = L1Penalty(0.5)\n",
    "if (not np.allclose(l1(test_params_1), true_l1_cost_1) or\n",
    "        not np.allclose(l1(test_params_2), true_l1_cost_2)):\n",
    "    print('L1Penalty.__call__ giving incorrect value(s).')\n",
    "elif (not np.allclose(l1.grad(test_params_1), true_l1_grad_1) or \n",
    "          not np.allclose(l1.grad(test_params_2), true_l1_grad_2)):\n",
    "    print('L1Penalty.grad giving incorrect value(s).')\n",
    "else:\n",
    "    print('All test values calculated correctly for L1Penalty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test values calculated correctly for L2Penalty\n"
     ]
    }
   ],
   "source": [
    "test_params_1 = np.array([[0.5, 0.3, -1.2, 5.8], [0.2, -3.1, 4.9, -5.0]])\n",
    "test_params_2 = np.array([0.8, -0.6, -0.3, 1.5, 2.8])\n",
    "true_l2_cost_1 = 23.52\n",
    "true_l2_grad_1 = np.array([[0.25, 0.15, -0.6, 2.9], [0.1, -1.55, 2.45, -2.5]])\n",
    "true_l2_cost_2 = 2.795\n",
    "true_l2_grad_2 = np.array([0.4, -0.3, -0.15, 0.75, 1.4])\n",
    "l2 = L2Penalty(0.5)\n",
    "if (not np.allclose(l2(test_params_1), true_l2_cost_1) or\n",
    "        not np.allclose(l2(test_params_2), true_l2_cost_2)):\n",
    "    print('L2Penalty.__call__ giving incorrect value(s).')\n",
    "elif (not np.allclose(l2.grad(test_params_1), true_l2_grad_1) or \n",
    "          not np.allclose(l2.grad(test_params_2), true_l2_grad_2)):\n",
    "    print('L2Penalty.grad giving incorrect value(s).')\n",
    "else:\n",
    "    print('All test values calculated correctly for L2Penalty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Training with regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in the second laboratory you implemented a function `grads_wrt_params`  to calculate the gradient of an error function with respect to the parameters of an affine model (layer), given gradients of the error function with respect to the model (layer) outputs.\n",
    "\n",
    "If we are training a model using a regularised objective function, we need to additionally calculate the gradients of the regularisation penalty terms with respect to the parameters and add these to the error function gradient terms. Following from the definition of the regularised objective $E^\\star$ above we have that the gradient of the overall objective with respect to the $d$th element of the $i$th model parameter is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E^\\star}{\\partial p^{(i)}_d} =\n",
    "    \\frac{\\partial \\bar{E}}{\\partial p^{(i)}_d} + \n",
    "    \\frac{\\partial C}{\\partial p^{(i)}_d}\n",
    "\\end{equation}\n",
    "\n",
    "We have already discussed in the second lab notebook how to calculate the error function gradient term $\\frac{\\partial \\bar{E}}{\\partial p^{(i)}_d}$. As the model complexity term is composed of a sum of per parameter terms and only one of these will depend on the $i$th parameter we can write\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C}{\\partial p^{(i)}_d} = \\frac{\\partial C^{(i)}}{\\partial p^{(i)}_d}\n",
    "\\end{equation}\n",
    "\n",
    "which corresponds to the penalty term gradients you implemented above. To enable us to use the same `Optimiser` implementation that we have previously used to train models without regularisation, we have altered the implementation of the `AffineLayer` class to allow us to specify penalty terms on the weight matrix and bias vector when creating an instance of the class and to add the corresponding penalty gradients to the returned value from the `grads_wrt_params` method. \n",
    "\n",
    "The penalty terms need to be specified as a class matching the interface of the `L1Penalty` and `L2Penalty` classes you implemented above, defining both a `__call__` method to calculate the penalty value for a parameter and a `grad` method to calculate the gradient of the penalty with respect to the parameter. Separate penalties can be specified for the weight and bias parameters, with it common to only regularise the weight parameters. \n",
    "\n",
    "The penalty terms for a layer are specifed using the `weights_penalty` and `biases_penalty` arguments to the `__init__` method of the `AffineLayer` class. If either (or both) ofthese are set to `None` (the default) no regularisation is applied to the corresponding parameter.\n",
    "\n",
    "Using the `L1Penalty` and `L2Penalty` classes you implemented in the previous exercise, train models to classify MNIST digit images with\n",
    "\n",
    "  * no regularisation\n",
    "  * an L1 penalty with coefficient $10^{-5}$ on the all of the weight matrix parameters\n",
    "  * an L1 penalty with coefficient $10^{-3}$ on the all of the weight matrix parameters\n",
    "  * an L2 penalty with coefficient $10^{-4}$ on the all of the weight matrix parameters\n",
    "  * an L2 penalty with coefficient $10^{-2}$ on the all of the weight matrix parameters\n",
    "  \n",
    "The models should all have three affine layers each interspersed with a `ReluLayer` (as implemented in the Lab 3 on Multiple Layer Models) and intermediate layers between the input and output should have dimensionalities of 100. The final output layer should be an `AffineLayer` (the model outputting the logarithms of the non-normalised class probabilties) and you should use the `CrossEntropySoftmaxError` as the error function (which calculates the softmax of the model outputs to convert to normalised class probabilities before calculating the corresponding multi-class cross entropy error). \n",
    "\n",
    "Use the `GlorotInit` class (which implements the weight initialisation technique discussed in the [fifth lecture](http://www.inf.ed.ac.uk/teaching/courses/mlp/2018-19/mlp05-learn.pdf)) to initialise the weights in all layers, using a gain of 0.5 (this adjusts for the fact that the rectified linear sets zeros all negative inputs), and initialises the biases to zero with a `ConstantInit` object. \n",
    "\n",
    "As an example the necessary parameter initialisers, model and error can be defined using\n",
    "\n",
    "```python\n",
    "weights_init = GlorotUniformInit(0.5, rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, \n",
    "                biases_init, weights_penalty=weights_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, \n",
    "                biases_init, weights_penalty=weights_penalty),\n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, \n",
    "                biases_init, weights_penalty=weights_penalty)\n",
    "])\n",
    "error = CrossEntropySoftmaxError()\n",
    "```\n",
    "\n",
    "This assumes all the relevant classes have been imported from their modules, a penalty object has been assigned to `weights_penalty` and a seeded random number generator assigned to `rng`.\n",
    "\n",
    "For each regularisation scheme, train the model for 100 epochs with a batch size of 50 and using a gradient descent with momentum learning rule with learning rate 0.01 and momentum coefficient 0.9. For each regularisation scheme you should store the run statistics (output of `Optimiser.train`) and the final values of the first layer weights for each of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training set error against epoch number for all different regularisation schemes on the same axis. On a second axis plot the validation set error against epoch number for all the different regularisation schemes. Interpret and comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines two functions for visualising the first layer weights of the models trained above. The first plots a histogram of the weight values and the second plots the first layer weights as feature maps, i.e. each row of the first layer weight matrix (corresponding to the weights going from the input MNIST image to a particular first layer output) is visualised as a $28\\times 28$ image. In these feature maps white corresponds to negative weights, black to positive weights and grey to weights close to zero. \n",
    "\n",
    "Use these functions to plot a histogram and feature map visualisation for the first layer weights of each model trained above. You should try to interpret the plots in the context of what you were told in the lecture about the behaviour of L1 versus L2 regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_histogram(param, fig_size=(6, 3), interval=[-1.5, 1.5]):\n",
    "    \"\"\"Plots a normalised histogram of an array of parameter values.\"\"\"\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(param.flatten(), 50, interval, normed=True)\n",
    "    ax.set_xlabel('Parameter value')\n",
    "    ax.set_ylabel('Normalised frequency density')\n",
    "    return fig, ax\n",
    "\n",
    "def visualise_first_layer_weights(weights, fig_size=(5, 5)):\n",
    "    \"\"\"Plots a grid of first layer weights as feature maps.\"\"\"\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    num_feature_maps = weights.shape[0]\n",
    "    grid_size = int(num_feature_maps**0.5)\n",
    "    max_abs = np.abs(model.params[0]).max()\n",
    "    tiled = -np.ones((30 * grid_size, \n",
    "                      30 * num_feature_maps // grid_size)) * max_abs\n",
    "    for i, fm in enumerate(model.params[0]):\n",
    "        r, c = i % grid_size, i // grid_size\n",
    "        tiled[1 + r * 30:(r + 1) * 30 - 1, \n",
    "              1 + c * 30:(c + 1) * 30 - 1] = fm.reshape((28, 28))\n",
    "    ax = fig.add_subplot(111)\n",
    "    max_abs = np.abs(tiled).max()\n",
    "    ax.imshow(tiled, cmap='Greys', vmin=-max_abs, vmax=max_abs)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Random data augmentation\n",
    "\n",
    "Another technique mentioned in the lectures for trying to reduce overfitting is to artificially augment the training data set by performing random transformations to the original training data inputs. The idea is to produce further artificial inputs corresponding to the same target class as the original input. The closer the artificially generated inputs are to the appearing like the true inputs the better as they provide more realistic additional examples for the model to learn from.\n",
    "\n",
    "For the handwritten image inputs in the MNIST dataset, an obvious way to considering augmenting the dataset is to apply small rotations to the original images. Providing the rotations are small we would generally expect that what we would identify as the class of a digit image will remain the same.\n",
    "\n",
    "Implement a function which given a batch of MNIST images as 784-dimensional vectors, i.e. an array of shape `(batch_size, 784)`\n",
    "\n",
    "  * chooses 25% of the images in the batch at random\n",
    "  * for each image in the 25% chosen, rotates the image by a random angle in $\\left[-30^\\circ,\\,30^\\circ\\right]$\n",
    "  * returns a new array of size `(batch_size, 784)` in which the rows corresponding to the 25% chosen images are the vectors corresponding to the new randomly rotated images, while the remaining rows correspond to the original images.\n",
    "  \n",
    "You will need to make use of the [`scipy.ndimage.interpolation.rotate`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.ndimage.interpolation.rotate.html#scipy.ndimage.interpolation.rotate) function which is imported below for you. For computational efficiency you should use bilinear interpolation by setting `order=1` as a keyword argument to this function rather than using the default of bicubic interpolation. Additionally you should make sure the original shape of the images is maintained by passing a `reshape=False` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import rotate\n",
    "\n",
    "def random_rotation(inputs, rng):\n",
    "    \"\"\"Randomly rotates a subset of images in a batch.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input image batch, an array of shape (batch_size, 784).\n",
    "        rng: A seeded random number generator.\n",
    "        \n",
    "    Returns:\n",
    "        An array of shape (batch_size, 784) corresponding to a copy\n",
    "        of the original `inputs` array with the randomly selected\n",
    "        images rotated by a random angle. The original `inputs`\n",
    "        array should not be modified.\n",
    "    \"\"\"\n",
    "    # Choose 25% of the images at random\n",
    "    random_rows = np.random.choice(inputs.shape[0], 25, replace=False)\n",
    "    random_images = inputs[random_rows]\n",
    "    \n",
    "    random_angle = np.random.rand(-30, 30)\n",
    "    \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to test your implementation. This uses the `show_batch_of_images` function we implemented in the first lab notebook to visualise the images in a batch before and after application of the random rotation transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 784)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-144b0e4471a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_batch_of_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtransformed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_batch_of_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0a0f009c43b0>\u001b[0m in \u001b[0;36mrandom_rotation\u001b[0;34m(inputs, rng)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mlp.data_providers import MNISTDataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "def show_batch_of_images(img_batch, fig_size=(3, 3)):\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    batch_size, im_height, im_width = img_batch.shape\n",
    "    # calculate no. columns per grid row to give square grid\n",
    "    grid_size = int(batch_size**0.5)\n",
    "    # intialise empty array to tile image grid into\n",
    "    tiled = np.empty((im_height * grid_size, \n",
    "                      im_width * batch_size // grid_size))\n",
    "    # iterate over images in batch + indexes within batch\n",
    "    for i, img in enumerate(img_batch):\n",
    "        # calculate grid row and column indices\n",
    "        r, c = i % grid_size, i // grid_size\n",
    "        tiled[r * im_height:(r + 1) * im_height, \n",
    "              c * im_height:(c + 1) * im_height] = img\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(tiled, cmap='Greys') #, vmin=0., vmax=1.)\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "test_data = MNISTDataProvider('test', 100, rng=rng)\n",
    "inputs, targets = test_data.next()\n",
    "_ = show_batch_of_images(inputs.reshape((-1, 28, 28)))\n",
    "transformed_inputs = random_rotation(inputs, rng)\n",
    "_ = show_batch_of_images(transformed_inputs.reshape((-1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Training with data augmentation\n",
    "\n",
    "One simple way to use data augmentation is to just statically augment the training data set - for example we could iterate through the training dataset applying a transformation function like that implemented above to generate new artificial inputs, and use both the original and newly generated data in a new data provider object. We are quite limited however in how far we can augment the dataset by with a static method like this however - if we wanted to apply 9 random rotations to each image in the original datase, we would end up with a dataset with 10 times the memory requirements and that would take 10 times as long to run through each epoch.\n",
    "\n",
    "An alternative is to randomly augment the data on the fly as we iterate through the data provider in each epoch. In this method a new data provider class can be defined that inherits from the original data provider to be augmented, and provides a new `next` method which applies a random transformation function like that implemented in the previous exercise to each input batch before returning it. This method means that on every epoch a different set of training examples are provided to the model and so in some ways corresponds to an 'infinite' data set (although the amount of variability in the dataset will still be significantly less than the variability in all possible digit images). Compared to static augmentation, this dynamic augmentation scheme comes at the computational cost of having to apply the random transformation each time a new batch is provided. We can vary this overhead by changing the proportion of images in a batch randomly transformed.\n",
    "\n",
    "An implementation of this scheme has been provided for the MNIST data set in the `AugmentedMNISTDataProvider` object in the `mlp.data_providers` module. In addition to the arguments of the original `MNISTDataProvider.__init__` method, this additional takes a `transformer` argument, which should be a function which takes as arguments an inputs batch array and a random number generator object, and returns an array corresponding to a random transformation of the inputs. \n",
    "\n",
    "Train a model with the same architecture as in exercise 3 and with no L1 / L2 regularisation using a training data provider which randomly augments the training images using your `random_rotation` transformer function. Plot the training and validation set errors over the training epochs and compare this plot to your previous results from exercise 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.data_providers import AugmentedMNISTDataProvider\n",
    "\n",
    "aug_train_data = AugmentedMNISTDataProvider('train', rng=rng, transformer=random_rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
